{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I always like to structure my imports into Python's native libs,\n",
    "# stuff I installed via conda/pip and local file imports (but we don't have those here)\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization related imports\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "\n",
    "# Main computation libraries\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "# Deep learning related imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR_PATH: ../data/MOOCCubeX\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Contains constants needed for data loading and visualization.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import enum\n",
    "\n",
    "\n",
    "# Supported datasets - only MOOCCubeX in this notebook\n",
    "class DatasetType(enum.Enum):\n",
    "    MOOCCubeX = 0\n",
    "\n",
    "    \n",
    "class GraphVisualizationTool(enum.Enum):\n",
    "    NETWORKX = 0,\n",
    "    IGRAPH = 1\n",
    "\n",
    "\n",
    "# We'll be dumping and reading the data from this directory\n",
    "DATA_DIR_PATH = '../data'\n",
    "MCX_PATH = os.path.join(DATA_DIR_PATH, 'MOOCCubeX')  # this is checked-in no need to make a directory\n",
    "print(f\"DATA_DIR_PATH: {MCX_PATH}\")\n",
    "\n",
    "TRAIN_RANGE = [0, 33417870]\n",
    "TEST_RANGE = [33417870, 134580654]\n",
    "\n",
    "# CORA_NUM_INPUT_FEATURES = ?\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "NUM_USERS = 694529\n",
    "\n",
    "USER = 0\n",
    "COURSE = 1\n",
    "node_class_to_color_map = {0: \"black\", 1: \"blue\"}\n",
    "edge_label_to_color_map = {0: \"red\", 1: \"green\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_read(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        data = np.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "def pickle_read(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "def numpy_save(path, data):\n",
    "    with open(path, 'wb') as file:\n",
    "        np.save(file, data)\n",
    "        \n",
    "def pickle_save(path, data):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_items = pickle_read(os.path.join(MCX_PATH, 'train_items_4.pkl'))\n",
    "# test_items = pickle_read(os.path.join(MCX_PATH, 'test_items_50.pkl'))\n",
    "# train_embeddings = pickle_read(os.path.join(MCX_PATH, 'train_embeddings_4_64.pkl'))\n",
    "# test_embeddings = pickle_read(os.path.join(MCX_PATH, 'test_embeddings_50_64.pkl'))\n",
    "\n",
    "# max(test_items)\n",
    "# len(train_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings[0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create User Nodes and Item Nodes\n",
    "# import networkx as nx\n",
    "# G = nx.Graph()\n",
    "\n",
    "# train_users = pickle_read(os.path.join(MCX_PATH, 'train_users_4.pkl'))\n",
    "# distinct_train_users = set(train_users)\n",
    "# train_labels = pickle_read(os.path.join(MCX_PATH, 'train_ratings_4.pkl'))\n",
    "\n",
    "# max_user_id = max(distinct_train_users)\n",
    "\n",
    "\n",
    "# print(f\"max_user_id: {max_user_id}\")\n",
    "\n",
    "# print(f\"len(train_users): {len(train_users)}\")\n",
    "\n",
    "# max_user_id\n",
    "# train_items = pickle_read(os.path.join(MCX_PATH, 'train_items_4.pkl'))\n",
    "# ajusted_train_items = list(map(lambda x: x+max_user_id+1, train_items))\n",
    "\n",
    "# distinct_train_items = set(ajusted_train_items)\n",
    "\n",
    "# nodes = distinct_train_users.union(distinct_train_items)\n",
    "\n",
    "# print(f\"len(nodes): {len(nodes)}\", max(nodes))\n",
    "\n",
    "# for node in tqdm(nodes):\n",
    "#     G.add_node(node)\n",
    "\n",
    "\n",
    "# for i in tqdm(range(len(train_users))):\n",
    "#     if train_labels[i] == 1:\n",
    "#         G.add_edge(train_users[i], ajusted_train_items[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Edge Labels\n",
    "# train_labels = pickle_read(os.path.join(MCX_PATH, 'train_ratings_4.pkl'))\n",
    "# test_labels = pickle_read(os.path.join(MCX_PATH, 'test_ratings_50.pkl'))\n",
    "# all_labels = train_labels + test_labels\n",
    "# npy_labels = np.array(all_labels, dtype=np.int32)\n",
    "# np.save(os.path.join(MCX_PATH, 'labels.npy'), npy_labels)\n",
    "# npy_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes = np.array(list(nodes), dtype=np.int32)\n",
    "# edges = np.array(edges, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy_save(os.path.join(MCX_PATH, 'nodes.npy'), nodes)\n",
    "# numpy_save(os.path.join(MCX_PATH, 'edges.npy'), edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RANGE = [0, 33417870]\n",
    "# nodes = numpy_read(os.path.join(MCX_PATH, 'nodes.npy'))\n",
    "# edges =numpy_read(os.path.join(MCX_PATH, 'edges.npy'))\n",
    "# nodes.shape[0], max(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Node Features\n",
    "# BERT_FEATURES_SIZE = 768\n",
    "# node_features = np.zeros((nodes.shape[0], BERT_FEATURES_SIZE))\n",
    "\n",
    "# for i in tqdm(range(len(train_items))):\n",
    "#     item_id = train_items[i] + NUM_USERS\n",
    "#     node_features[item_id] = train_embeddings[i].numpy()\n",
    "# node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# import scipy.sparse\n",
    "# node_features_csr = scipy.sparse.csc_matrix(node_features)\n",
    "\n",
    "# pickle_save(os.path.join(MCX_PATH, 'node_features_csr.pkl'), node_features_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges = np.transpose(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll pass the training config dictionary a bit later\n",
    "def load_graph_data(training_config, device):\n",
    "    dataset_name = training_config['dataset_name'].lower()\n",
    "    should_visualize = training_config['should_visualize']\n",
    "    just_train = training_config['just_train']\n",
    "\n",
    "    if dataset_name == DatasetType.MOOCCubeX.name.lower():\n",
    "\n",
    "        # shape = (N, 768), where N is the number of nodes\n",
    "        node_features = pickle_read(os.path.join(MCX_PATH, 'node_features_csr.pkl')).todense()\n",
    "        # shape = (E, 2) \n",
    "        edges = numpy_read(os.path.join(MCX_PATH, 'edges.npy'))\n",
    "        if just_train:\n",
    "            edges = edges[TRAIN_RANGE[0]:TRAIN_RANGE[1]]\n",
    "        # shape = (E,)\n",
    "        edge_labels = numpy_read(os.path.join(MCX_PATH, 'labels.npy'))\n",
    "        if just_train:\n",
    "            edge_labels = edge_labels[TRAIN_RANGE[0]:TRAIN_RANGE[1]]\n",
    "\n",
    "        num_of_nodes = node_features.shape[0]\n",
    "\n",
    "        # shape = (2, E), where E is the number of edges, and 2 for source and target nodes. Basically edge index\n",
    "        # contains tuples of the format S->T, e.g. 0->3 means that node with id 0 points to a node with id 3.\n",
    "        topology = np.transpose(edges)\n",
    "\n",
    "        # Note: topology is just a fancy way of naming the graph structure data \n",
    "        # (aside from edge index it could be in the form of an adjacency matrix)\n",
    "\n",
    "        if should_visualize:  # network analysis and graph drawing\n",
    "            plot_in_out_degree_distributions(topology, num_of_nodes, dataset_name)  # we'll define these in a second\n",
    "            visualize_graph(topology, edge_labels, dataset_name)\n",
    "\n",
    "        # Convert to dense PyTorch tensors\n",
    "\n",
    "        # Needs to be long int type because later functions like PyTorch's index_select expect it\n",
    "        topology = torch.tensor(topology, dtype=torch.long, device=device)\n",
    "        edge_labels = torch.tensor(edge_labels, dtype=torch.long, device=device)  # Cross entropy expects a long int\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Indices that help us extract nodes that belong to the train/val and test splits\n",
    "        train_indices = torch.arange(TRAIN_RANGE[0], TRAIN_RANGE[1], dtype=torch.long, device=device)\n",
    "        test_indices = torch.arange(TEST_RANGE[0], TEST_RANGE[1], dtype=torch.long, device=device)\n",
    "\n",
    "        return node_features, edge_labels, topology, train_indices, test_indices\n",
    "    else:\n",
    "        raise Exception(f'{dataset_name} not yet supported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just define dummy visualization functions for now - just to stop Python interpreter from complaining!\n",
    "# We'll define them in a moment, properly, I swear.\n",
    "\n",
    "# def plot_in_out_degree_distributions():\n",
    "#     pass\n",
    "\n",
    "# def visualize_graph():\n",
    "#     pass\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# config = {\n",
    "#     'dataset_name': DatasetType.MOOCCubeX.name,\n",
    "#     'should_visualize': False,\n",
    "#     'just_train': True\n",
    "# }\n",
    "# node_features, edge_labels, edge_index, train_indices, test_indices = load_graph_data(config, device)\n",
    "\n",
    "# print(node_features.shape, node_features.dtype)\n",
    "# print(edge_labels.shape, edge_labels.dtype)\n",
    "# print(edge_index.shape, edge_index.dtype)\n",
    "# print(train_indices.shape, train_indices.dtype)\n",
    "# print(test_indices.shape, test_indices.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_in_out_degree_distributions(edge_index, num_of_nodes, dataset_name):\n",
    "    \"\"\"\n",
    "        Note: It would be easy to do various kinds of powerful network analysis using igraph/networkx, etc.\n",
    "        I chose to explicitly calculate only the node degree statistics here, but you can go much further if needed and\n",
    "        calculate the graph diameter, number of triangles and many other concepts from the network analysis field.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(edge_index, torch.Tensor):\n",
    "        edge_index = edge_index.cpu().numpy()\n",
    "        \n",
    "    assert isinstance(edge_index, np.ndarray), f'Expected NumPy array got {type(edge_index)}.'\n",
    "\n",
    "    # Store each node's input and output degree (they're the same for undirected graphs such as Cora)\n",
    "\n",
    "    user_degree = np.zeros(NUM_USERS, dtype=int)\n",
    "    course_degree = np.zeros(num_of_nodes - NUM_USERS - 1, dtype=int)\n",
    "\n",
    "    # Edge index shape = (2, E), the first row contains the source nodes, the second one target/sink nodes\n",
    "    # Note on terminology: source nodes point to target/sink nodes\n",
    "    num_of_edges = edge_index.shape[1]\n",
    "    for cnt in tqdm(range(num_of_edges)):\n",
    "        source_node_id = edge_index[0, cnt]\n",
    "        target_node_id = edge_index[1, cnt]\n",
    "\n",
    "        user_degree[source_node_id - 1] += 1  # source node points towards some other node -> increment its out degree\n",
    "        course_degree[target_node_id - NUM_USERS - 1] += 1  # similarly here\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8), dpi=100)  # otherwise plots are really small in Jupyter Notebook\n",
    "    fig.subplots_adjust(hspace=0.6)\n",
    "\n",
    "    plt.subplot(311)\n",
    "    plt.plot(user_degree, color='red')\n",
    "    plt.xlabel('node id'); plt.ylabel('user-degree count'); plt.title('Degree for different node ids')\n",
    "\n",
    "    plt.subplot(312)\n",
    "    plt.plot(course_degree, color='green')\n",
    "    plt.xlabel('node id'); plt.ylabel('course-degree count'); plt.title('Out degree for different node ids')\n",
    "\n",
    "    plt.subplot(313)\n",
    "    plt.hist(course_degree, bins=50)\n",
    "    plt.xlabel('node degree')\n",
    "    plt.ylabel('# nodes for a given out-degree') \n",
    "    plt.title(f'Node course-degree distribution for {dataset_name} dataset')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_of_nodes = len(nodes)\n",
    "# plot_in_out_degree_distributions(edge_index, num_of_nodes, config['dataset_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nodes = torch.tensor(np.arange(0, 10, dtype=np.int32), dtype=torch.int64)\n",
    "# nodes\n",
    "# edges_1 = np.random.choice(10, 15)\n",
    "# edges_2 = np.random.choice(10, 15)\n",
    "# edges = np.array([edges_1, edges_2], dtype=np.int32)\n",
    "# edges = torch.tensor(edges, dtype=torch.int64)\n",
    "# edges.shape, nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_features = torch.rand(10, 20)\n",
    "# node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = torch.randint(0, 2, (15,))\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_indices = torch.tensor(range(0, 10), dtype=torch.int64)\n",
    "# train_indices\n",
    "# test_indices = torch.tensor(range(10,15), dtype=torch.int64)\n",
    "# test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = labels.index_select(0, train_indices)\n",
    "# test_labels = labels.index_select(0, test_indices)\n",
    "# graph_data = (node_features, edges)\n",
    "# train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# dir2 = os.path.abspath('')\n",
    "# dir1 = os.path.dirname(dir2)\n",
    "# if not dir1 in sys.path: sys.path.append(dir1)\n",
    "# from architectures.gat import GAT\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "# print(f'Using device: {device}')\n",
    "\n",
    "# graph_data = (node_features.to(device), edge_index.to(device))\n",
    "\n",
    "# config = {\n",
    "#     \"num_of_layers\": 2,  # GNNs, contrary to CNNs, are often shallow (it ultimately depends on the graph properties)\n",
    "#     \"num_heads_per_layer\": [8, 1],\n",
    "#     \"num_features_per_layer\": [768, 8, 1],\n",
    "#     \"add_skip_connection\": False, \n",
    "#     \"bias\": False,  # result is not so sensitive to bias\n",
    "#     \"dropout\": 0.6,  # result is sensitive to dropout\n",
    "# }\n",
    "\n",
    "# gat = GAT(\n",
    "#         num_of_layers=config['num_of_layers'],\n",
    "#         num_heads_per_layer=config['num_heads_per_layer'],\n",
    "#         num_features_per_layer=config['num_features_per_layer'],\n",
    "#         add_skip_connection=config['add_skip_connection'],\n",
    "#         bias=config['bias'],\n",
    "#         dropout=config['dropout'],\n",
    "#         log_attention_weights=False  # no need to store attentions, used only in playground.py for visualizations\n",
    "#     ).to(device)\n",
    "\n",
    "# preds = gat(graph_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds.shape, edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = edge_index.numpy().T\n",
    "# new_array = [tuple(row) for row in data]\n",
    "# uniques = set(new_array)\n",
    "# len(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEU illustrative example.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"NEU illustrative example.\"\"\"\n",
    "\n",
    "# import networkx as nx\n",
    "# from karateclub.node_embedding.meta import NEU\n",
    "# from karateclub.node_embedding.neighbourhood import GLEE\n",
    "\n",
    "# g = nx.newman_watts_strogatz_graph(69920, 20, 0.05)\n",
    "\n",
    "# model = GLEE(dimensions=64)\n",
    "\n",
    "# meta_model = NEU()\n",
    "\n",
    "# print(g.number_of_nodes(), g.number_of_edges())\n",
    "# meta_model.fit(g, model)\n",
    "# embs = meta_model.get_embedding()\n",
    "\n",
    "\n",
    "# # numpy_save(os.path.join(MCX_PATH, 'glee_embeddings.npy'), embs)\n",
    "# type(embs), embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# G = nx.from_edgelist(edges)\n",
    "# G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# from karateclub.node_embedding.meta import NEU\n",
    "# from karateclub.node_embedding.neighbourhood import GLEE\n",
    "# model = GLEE()\n",
    "\n",
    "# meta_model = NEU()\n",
    "\n",
    "# print(G.number_of_nodes(), G.number_of_edges())\n",
    "# meta_model.fit(G, model)\n",
    "# embs = meta_model.get_embedding()\n",
    "# numpy_save(os.path.join(MCX_PATH, 'glee_embeddings.npy'), embs)\n",
    "# type(embs), embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), (699229, 129), 360802164)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = numpy_read(os.path.join(MCX_PATH, 'glee_embeddings.npy')).astype(np.float32)\n",
    "numpy_save(os.path.join(MCX_PATH, 'glee_embeddings.npy'), embs)\n",
    "embs.dtype, embs.shape, embs.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_users): 33417870\n",
      "len(train_items): 33417870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33417870 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_id: 419025\n",
      "max_id: 652087\n",
      "max_id: 652968\n",
      "max_id: 665610\n",
      "max_id: 686482\n",
      "max_id: 691546\n",
      "max_id: 691561\n",
      "max_id: 691864\n",
      "max_id: 692061\n",
      "max_id: 692829\n",
      "max_id: 693474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 49945/33417870 [00:00<04:59, 111227.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_id: 693592\n",
      "max_id: 694182\n",
      "max_id: 694403\n",
      "max_id: 694450\n",
      "max_id: 694508\n",
      "max_id: 694510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 157626/33417870 [00:01<04:20, 127854.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_id: 694511\n",
      "max_id: 694512\n",
      "max_id: 694514\n",
      "max_id: 694519\n",
      "max_id: 694528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33417870/33417870 [03:06<00:00, 179256.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33417870"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_users = pickle_read(os.path.join(MCX_PATH, 'train_users_4.pkl'))\n",
    "print(f\"len(train_users): {len(train_users)}\")\n",
    "\n",
    "train_items = pickle_read(os.path.join(MCX_PATH, 'train_items_4.pkl'))\n",
    "print(f\"len(train_items): {len(train_items)}\")\n",
    "\n",
    "emb_matrix = []\n",
    "max_id = 0\n",
    "for i in tqdm(range(int(len(train_users)))):\n",
    "    user = train_users[i]\n",
    "    item = train_items[i] - NUM_USERS\n",
    "\n",
    "    if max_id < user or max_id < item + NUM_USERS:\n",
    "        max_id = max(max_id, user, item + NUM_USERS)\n",
    "        print(f\"max_id: {max_id}\")\n",
    "\n",
    "    user_emb = embs[user]\n",
    "    item_emb = embs[item]\n",
    "    cat_emb = np.concatenate((user_emb, item_emb), axis=0)\n",
    "    cat_emb = torch.tensor(cat_emb, dtype=torch.float32)\n",
    "    emb_matrix.append(cat_emb)\n",
    "    \n",
    "len(emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pickle_read(os.path.join(MCX_PATH, 'train_embeddings_4_64.pkl'))\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33417870/33417870 [00:04<00:00, 6705271.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(embeddings))):\n",
    "    if embeddings[i].nbytes != 3072:\n",
    "        print(embeddings[i].nbytes, len(embeddings), len(embs))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
